# Training configuration for the greenhouse test environment

training:
  # General training options
  max_training_steps: 10000

  # Reward function to use when stepping the environment
  # reward_fn: pyrobosim_ros_gym.envs.greenhouse.sparse_reward  # Try this first
  # reward_fn: pyrobosim_ros_gym.envs.greenhouse.dense_reward  # Try this second
  reward_fn: pyrobosim_ros_gym.envs.greenhouse.full_reward  # End with this one

  # Evaluations during training
  #   Every eval_freq steps, n_eval_episodes episodes will be run.
  #   If the mean cumulative reward is greater than reward_threshold,
  #   training will complete.
  eval:
    n_eval_episodes: 5
    eval_freq: 500
    reward_threshold: 7.0

  # Individual algorithm options
  DQN:
    gamma: 0.99
    exploration_initial_eps: 0.75
    exploration_final_eps: 0.05
    exploration_fraction: 0.2
    learning_starts: 25
    learning_rate: 0.0002
    batch_size: 16
    gradient_steps: 5
    train_freq: 4  # steps
    target_update_interval: 10
    policy_kwargs:
      activation_fn: torch.nn.ReLU
      net_arch: [16, 8]

  PPO:
    gamma: 0.99
    learning_rate: 0.0003
    batch_size: 8
    n_steps: 8
    policy_kwargs:
      activation_fn: torch.nn.ReLU
      net_arch:
        pi: [16, 8]  # actor size
        vf: [16, 8]  # critic size

  SAC:
    gamma: 0.99
    learning_rate: 0.0003
    batch_size: 16
    train_freq: 4  # steps
    gradient_steps: 5
    tau: 0.005
    target_update_interval: 10
    policy_kwargs:
      activation_fn: torch.nn.ReLU
      net_arch:
        pi: [16, 8]  # actor size
        qf: [16, 8]  # critic size (SAC uses qf, not vf)

  A2C:
    gamma: 0.99
    learning_rate: 0.0007
    policy_kwargs:
      activation_fn: torch.nn.ReLU
      net_arch:
        pi: [16, 8]  # actor size
        vf: [16, 8]  # critic size
